    首先是拿到的数据，因为是比赛，所以数据是可以自己下载的，省去了数据获取的部分，不过我用爬虫爬过一些金融网站的数据，比如团贷网，是关于融资的。还有大家关心的房价问题，比如链家网，上面有很多的房价数据。
    总得来说,数据在获取之后基本分为两种。
    一种是结构化数据，即直接提供原始样本特征数据，样本的特征已经给出，特征的维度通常比较大。这类的数据比较方便，因为基本特征已经给定了，所有的操作都基于这些特征。
    另一种给的是非结构化的数据，比如说微博的文本以及本条微博的相关数据，一个用户所有的微博。要求从这些数据中提取特征。那么这个就可能天差地别了，我记得在"新浪微博互动预测"中，我自己提取了大概25维的数据（这里说的是原始特征，还没做特征组合），自己跑起来还不错，后来在群里知道大牛特征都是几百几百的加，虽然特征不是越多越好，因为容易过拟合（这一个信息被重复使用了很多次），但是对于百级别的特征，多了总比少了好，电脑也完全能处理。但是我始终想不通是怎么提取到这么多特征的，后来在一次决赛现场答辩的时候，才体会到时间线的重要性。
    在拿到结构化数据之后，需要对数据进行一些分析，有最重要的有两点：
    一. 缺失值统计和处理。
    二. 样本的统计和处理。
    三. 样本不平衡。
    实践中的处理方法：
    1. 在样本和属性两个维度进行筛选，删除缺失率很大(大于80，或是rank后topk)的样本或属性。
    2. 如果样本的属性缺失率不是特别高，样本需要被保留，这个缺失值需要被填充，通常比较有效的方法是用一个固定值来填充，paper中有很多方法，比如用其他属性来预测，用其他不缺失该属性的样本的该属性来插值，但是这样的方法我用过数次，效果都不是很好，最好的方法就是直接用一个固定值来填充，后面还要进行特征的组合和筛选，可以剔除不合理的假设。
    3. 对于样本不平衡的问题，总的来说我用过三种方法：
	a)过采样
	b)降采样
	c)人工样本
    方法a和方法b都是简单的对样本进行采样，但是两者的不同在于过采样会造成样本信息的冗余，降采样会造成样本信息的缺失。如果在处理缺失值之后还存在大量样本，那么可以对不平衡样本进行降采样。但是通常的情况是，样本是以时间为线的，样本太多可以选择时效性较好的样本，也就是对时间线上的样本根据时间进行一个函数的采样，这个函数可以自己设计，所以我其实在最终方案中没有使用过降采样，因为样本的数量可控。过采样虽然会造成样本信息冗余，但是会保留全部的信息，信息留的越多，后面的事情可能越好做。
    对于人工样本来说，我用过一样次，但是效果不好，但是我看到有人用效果还行的，后来发现别人应该是在特征工程之后做的样本生成。因为很多差值是线性差值，而属性之间很可能是非线性关系，所以插值后的样本可能非常奇怪，以至于根本没有用，我在一次实践中插出来的样本仅仅和原样本相差很小的值，结果就和过采样一样，原因应该是属性之间也不是线性关系，用线性插值的话，可插值的空间很小。
    举个例子：
    在"新浪微博互动预测大赛"第二赛季中，要对样本的级别(1-5)进行预测，但是1类样本很多，5类样本很少，样本数量随级别快速下降，如果忽略样本比例，预测出来的值基本都是1类样本，这里的样本基数很大，完全可以进行降采样，但是在对比过采样方法之后，将样本按照比例复制成1:1:1:1:1,得到的效果比降采样好很多。
    其实我觉得这个问题一个更好的方法就是交给模型，模型在学习的时候考虑到样本比例，加大数量少的样本权值，让模型取处理这个问题，通常比上述方法都要好。如XGBOOT里设置scale_pos_weight的值。
    还有对数据其他的一些分析会对后面的预测有很大帮助。比如我做过的”微额信贷”比赛中，数据是一年的数据，但是由于微额信贷是刚刚兴起的一种金融产品，所以业务的调整可能比较大，在我对缺失值统计之后，发现根据样本的缺失值分类，样本可以明显的分成三个部分，那么这三个部分可能对应业务的三次大的调整，可以分开建模，可以分开处理特征工程，很多很多，对之后的工作都有很大帮助。
